{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "!pip3 install transformers\n",
    "pip3 install transformers==2.9.0\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from transformers import BartForSequenceClassification, BartTokenizer\n",
    "from torch.nn import functional as F\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import operator\n",
    "import pprint\n",
    "\n",
    "import pickle\n",
    "import dill"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ipywidgets import IntProgress"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained('deepset/sentence_bert')\n",
    "tf_model = TFAutoModel.from_pretrained('deepset/sentence_bert', from_pt=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "README.md      \u001b[34mdeploy\u001b[m\u001b[m         model.pt       \u001b[34mtf_code\u001b[m\u001b[m        \u001b[34mtourch_code\u001b[m\u001b[m\r\n",
      "\u001b[34mback\u001b[m\u001b[m           email-pt.ipynb model.pth      tf_model.h5\r\n",
      "config.json    email-tf.ipynb \u001b[34mmodel_files\u001b[m\u001b[m    tfmodel.pkl\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['unhappy', 'happy', \n",
    "           'positive', 'negative', 'neutral']\n",
    "\n",
    "sentence = '''\n",
    "Hi, \n",
    "I had very good experience in using the service.\n",
    "I have recently made use of Virtual Shopping to help me to rearrang home delivery for my order.\n",
    "Because I want to change the delivery date, can you share the instructions.\n",
    "Kindly regards,\n",
    "Lu\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode and tokenize the sentence and labels\n",
    "inputs = tokenizer.batch_encode_plus([sentence] + labels,\n",
    "                                     return_tensors='tf',\n",
    "                                     pad_to_max_length=True)\n",
    "input_ids = inputs['input_ids']\n",
    "attention_mask = inputs['attention_mask']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_outputs = tf_model(inputs)\n",
    "\n",
    "response = 'Sentence submitted for labelling \\n' + str(sentence)+'\\n'\n",
    "label_emb = tf.reduce_mean(tf_outputs[0][1:], 1)\n",
    "sentence_emb = tf.reduce_mean(tf_outputs[0][:1], 1)\n",
    "\n",
    "\n",
    "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "similarities = cosine_loss(sentence_emb, label_emb).numpy()\n",
    "\n",
    "result = dict(zip(labels, similarities))\n",
    "sorted_d = sorted(result.items(), key=operator.itemgetter(1))\n",
    "pprint.pprint(sorted_d)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "sentence_emb = tf_outputs[1][:1]\n",
    "label_emb = tf_outputs[1][1:]\n",
    "print(sentence_emb.shape)\n",
    "print(label_emb.shape)\n",
    "\n",
    "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1, reduction=tf.keras.losses.Reduction.NONE)\n",
    "similarities = cosine_loss(sentence_emb, label_emb).numpy()\n",
    "\n",
    "result = dict(zip(labels, similarities))\n",
    "sorted_d = sorted(result.items(), key=operator.itemgetter(1))\n",
    "pprint.pprint(sorted_d)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "md = pickle.load('tfmodel.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model.save_pretrained('/Users/napt/Workspace/gft/model-serving-examples/aiplatform/model_files/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls model_files\n",
    "from tensorflow import keras\n",
    "import os\n",
    "model_dir = '/Users/napt/Workspace/gft/model-serving-examples/aiplatform/model_files/'\n",
    "model = TFAutoModel.from_pretrained(model_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_outputs = model(inputs)\n",
    "\n",
    "response = 'Sentence submitted for labelling \\n' + str(sentence)+'\\n'\n",
    "label_emb = tf.reduce_mean(tf_outputs[0][1:], 1)\n",
    "sentence_emb = tf.reduce_mean(tf_outputs[0][:1], 1)\n",
    "\n",
    "\n",
    "cosine_loss = tf.keras.losses.CosineSimilarity(axis=1, reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "similarities = cosine_loss(sentence_emb, label_emb).numpy()\n",
    "\n",
    "result = dict(zip(labels, similarities))\n",
    "sorted_d = sorted(result.items(), key=operator.itemgetter(1))\n",
    "pprint.pprint(sorted_d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf_model = TFAutoModel.from_pretrained('/Users/napt/Workspace/gft/model-serving-examples/aiplatform/model_files/tf_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from transformers import BartForSequenceClassification, BartTokenizer\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "\n",
    "class EmailClassifier(object):\n",
    "    def __init__(self, model):\n",
    "        self._model = model\n",
    "        self.class_lables = ['unhappy', 'happy', 'positive', 'negative', 'neutral']\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, model_dir):\n",
    "        model_file = os.path.join(model_dir, 'tf_model.h5')\n",
    "        model = torch.load(model_file)\n",
    "        return cls(model)\n",
    "\n",
    "    def predict(self, sentence, **kwargs):\n",
    "        inputs = tokenizer.batch_encode_plus([sentence] + self.class_lables,\n",
    "                                             return_tensors='pt',\n",
    "                                             pad_to_max_length=True)\n",
    "        input_ids = inputs['input_ids']\n",
    "        attention_mask = inputs['attention_mask']\n",
    "        output = self._model(input_ids, attention_mask=attention_mask)[0]\n",
    "        sentence_emb = output[:1].mean(dim=1)\n",
    "        label_emb = output[1:].mean(dim=1)\n",
    "\n",
    "        similarities = F.cosine_similarity(sentence_emb, label_emb)\n",
    "\n",
    "        result = dict(zip(self.class_lables, similarities.tolist()))\n",
    "        predicted = sorted(result.items(), key=operator.itemgetter(1),\n",
    "                           reverse=True)\n",
    "\n",
    "        return predicted"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of predictor class, this is also available in tf_code folder.\n",
    "from transformers import AutoTokenizer, TFAutoModel\n",
    "from transformers import BartForSequenceClassification, BartTokenizer\n",
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "class MyPredictor(object):\n",
    "    \"\"\"An example Predictor for an AI Platform custom prediction routine.\"\"\"\n",
    "\n",
    "    def __init__(self, model):\n",
    "\n",
    "        self._model = model\n",
    "        self.class_lables = ['unhappy', 'happy', 'positive', 'negative',\n",
    "                             'neutral']\n",
    "\n",
    "    def predict(self, instances, **kwargs):\n",
    "        tokenizer = AutoTokenizer.from_pretrained('deepset/sentence_bert')\n",
    "        inputs = tokenizer.batch_encode_plus([sentence] + self.class_lables,\n",
    "                                             return_tensors='pt',\n",
    "                                             pad_to_max_length=True)\n",
    "        tf_outputs = model(inputs)\n",
    "\n",
    "        label_emb = tf.reduce_mean(tf_outputs[0][1:], 1)\n",
    "        sentence_emb = tf.reduce_mean(tf_outputs[0][:1], 1)\n",
    "\n",
    "        cosine_loss = tf.keras.losses.CosineSimilarity(axis=1,\n",
    "                                                       reduction=tf.keras.losses.Reduction.NONE)\n",
    "\n",
    "        similarities = cosine_loss(sentence_emb, label_emb).numpy()\n",
    "\n",
    "        result = dict(zip(labels, similarities))\n",
    "        #predicted = sorted(result.items(), key=operator.itemgetter(1))\n",
    "\n",
    "        return result\n",
    "\n",
    "    @classmethod\n",
    "    def from_path(cls, model_dir):\n",
    "        model = TFAutoModel.from_pretrained(model_dir)\n",
    "        return cls(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of using sklearn for cosine_similarity\n",
    "\n",
    "response = 'Sentence submitted for labelling \\n' + str(sentence)+'\\n'\n",
    "label_emb = tf.reduce_mean(tf_outputs[0][1:], 1)\n",
    "sentence_emb = tf.reduce_mean(tf_outputs[0][:1], 1)\n",
    "\n",
    "\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "cosine_sim = cosine_similarity(sentence_emb.numpy(), label_emb.numpy()).flatten()\n",
    "related_doc_indices = cosine_sim.argsort()[:-len(labels)-1:-1]\n",
    "for ind in related_doc_indices:\n",
    "    print('label: '+labels[ind], '    \\t similarity: '+str(round(cosine_sim[ind], 2)) )"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
