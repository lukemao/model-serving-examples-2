- ~~General comment, I think that there should be a discussion as to why AI platform is not used in this blog. Why would we use these services instead? Is it because we might want to integrate with things like Cloud Memory Store ? Or is it because we might want to include custom tuning or is it because there is some performance issue or limitation. Is there some way around the matrix multiplication issue? What about GPU’s/TPU’s ? Perhaps this gets too complex so we should explicitly exclude them from the discussion – but we need a reason really, or we could say that that’s in the next blog?~~
- ~~General comment, there should be some references included – for example “all you need is attention” paper, the word2vec paper, T5 paper (because you use T5 so much)~~
~~- General comment,  You need to be more precise about the findings – for example on the pricing section…. so what! My conclusion is that for bursty applications where individual request latency is not the most important priority then cloud run could be a lot cheaper… but I guess the question is if we have a big file or constant stream of requests (say 2 per minute) right through a day, and some delay on processing (say 4 hrs) is acceptable then is there a point where setting up a GAE will come out a lot cheaper? How many requests does it take for GCR and CF to price themselves out vs a GAE instance? At what burstiness does GAE get too expensive? Cold starts issue – again at what point does this matter ? I think we need to get to a statement like “for more than a max of n invocations per minute throughput and less than m invocations per day Cloud Run is always the most effective choice” or similar.~~
 

Detailed comments :
- ~~“we ran 10 to 20 model prediction requests” – how many exactly? Why was this the right number?~~
- ~~For cloud run you reference the zeroshot-t5 folder, is this a good idea? Would it be better to just stick to the figures and inclusions in the blog? (genuine question!)~~
- ~~Horizontal vs vertical comparisons: I found this a bit confusing. Perhaps say “comparing the different services” and “scaling each service”? I had to keep checking what the horizontal and vertical meant.~~
- ~~Figures should be numbered so it’s easier to refer to them in discussion.~~
- ~~For latency tests the axis titles are clipped off the figures. All figures should have title, x and y axis labels & x and y axis titles, and captions  such as (Figure 1. The caption for this figure that says something about it like, note that there are outliers for cloud functions.)~~
- ~~Define tightly latency, reliability, robustness and throughput.~~
- ~~I think that you interchange reliability and robustness in the text – stick to one word (either is fine!)~~
- ~~I don’t think you should say “Cloud Run is the winner” instead it’s better to say “in this test we observe that Cloud Run has a low variation in response time, while GAE has high variation. In applications of interest to our team this kind of predictable response time is highly desirable, and although GAE does sometimes return a result with lower latency than Cloud Run its overall performance would tend to make it less desirable”. (or similar but better!)~~
- ~~Not quite sure what is meant by efficiency – do you mean memory footprint? I am not sure that there are enough configurations tested to make a claim.~~
- ~~For throughput tests, is there an Cloud Functions 8GB no file system option? Is it the File System or the memory that makes the difference?~~

- ~~What is exactly meant by latency in the blog? In the Latency Tests section I thought that this meant the time for a response from the service to the query made. In the vertical tests there are four numbers, min, mean, max and latency and you say that latency is the “difference between min and max response time” surely that’s the variation ? It might be less confusing just to leave that out (the yellow bars) and comment on it in the text.~~
- ~~Including the monitoring info is great – It would be good to separate the figures/screen shots a bit more possibly with a bit of commentary on each one. Rephrase “App Engine can be a bit lagging compared..” to “In our experience Cloud Run and Cloud Functions monitors provide close to real time feeback on the service performance and behaviour while App Engine metrics are generally provided with higher latency making them more suited to post-hoc inspection and review.”~~
- ~~Quotas and limits – like this section. I think you should put a sentence that says “obviously these change and might be updated at any time by Google so check for yourselves”~~
~~- Conclusion : I think that if you can tighten up on the pricing section (work out the bangs for bucks / limitations things a bit more) then this could be reworked in the light of that to be more precise too. At the end rephrase from “As implied” on to : “The work that we have done on customer’s business problems using the new generation of language models has shown us that deployment choices are a critical and complex part of what we need to deliver to build something that’s really successful. What we’ve presented in this blog is just a fragment of the picture, and there’s a tremendous amount of art and experiment that needs to be created covering the options for (just as examples) different application types and on different cloud platforms.  We are excited to be pushing forward with this work and hope to be able to write further posts on this in the future.”, or something similar?~~